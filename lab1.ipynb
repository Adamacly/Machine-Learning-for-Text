{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9af7be",
   "metadata": {},
   "source": [
    "# Lab : Generative models at the lexical level\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "Explore the two generative models seen in class (Naïve Bayes for classification, Latent Dirichlet Allocation for topic modeling) by applying them to a relatively small classification dataset - **20NewsGroup** - try to  look at how they perform on the classification task and understand how to interpret the topic models. \n",
    "1. Pre-process the data: clean it, understand the various possibilities for pre-processing steps.\n",
    "2. Obtain representations: first, symbolic document representations: **BoW**, then **TF-IDF**\n",
    "    - We will first implement our functions for doing so, then use ```sklearn```. \n",
    "3. Perform classification:\n",
    "    - We will first implement our function for Naïve Bayes, then use ```sklearn```.\n",
    "    - We will search for the best hyper-parameters using ```pipeline```.\n",
    "4. Perform topic modeling:\n",
    "    - We will quickly compare LSA and LDA and try to interpret them. \n",
    "    - We will implement simple metrics and look for the best hype-parameters maximizing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7af15",
   "metadata": {},
   "source": [
    "## Necessary dependancies\n",
    "\n",
    "We will need the following packages:\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a147280",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "We retrieve the textual data in the variable *texts*.\n",
    "\n",
    "The labels are retrieved in the variable $y$ - it contains *len(texts)* of them: $0$ indicates that the corresponding review is negative while $1$ indicates that it is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebe3f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9340ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train = fetch_20newsgroups(subset='train',\n",
    "                              remove=('headers', 'footers', 'quotes')\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8702195",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dir(ng_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32113108",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ng_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0605d5b",
   "metadata": {},
   "source": [
    "Example of one document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83da5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ng_train.data[0])\n",
    "print(\"Target: \", ng_train.target_names[ng_train.target[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbce6d4",
   "metadata": {},
   "source": [
    "## 1 - Document Preprocessing\n",
    "\n",
    "You should use a pre-processing function you can apply to the raw text before any other processing (*i.e*, tokenization and obtaining representations). Some pre-processing can also be tied with the tokenization (*i.e*, removing stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e076e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde39413-3bb3-4140-bdd5-2f982623493b",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str,\n",
    "               rm_numbers=True,\n",
    "               rm_punct=True,\n",
    "               rm_stop_words=True,\n",
    "               rm_short_words=True):\n",
    "    # make lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove URLs\n",
    "    URL_PATTERN = re.compile(r'\\b(?:https?://|www\\.)\\S+\\b', flags=re.IGNORECASE)\n",
    "    #URL_PATTERN.sub('', text)\n",
    "\n",
    "    # remove domain names\n",
    "    DOMAIN_PATTERN = re.compile(r'\\b(?:[a-z0-9-]+\\.)+[a-z]{2,}\\b', flags=re.IGNORECASE)\n",
    "    #DOMAIN_PATTERN.sub('', text)\n",
    "    \n",
    "    # remove email addresses\n",
    "    EMAIL_PATTERN = re.compile(r'\\b[a-z0-9._%+-]+@(?:[a-z0-9-]+\\.)+[a-z]{2,}\\b', flags=re.IGNORECASE)\n",
    "    #EMAIL_PATTERN.sub('', text)\n",
    "                               \n",
    "    # remove punctuation\n",
    "    if rm_punct:\n",
    "        text = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "\n",
    "    # remove numbers\n",
    "    if rm_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # replace linebreaks and strip\n",
    "    text = ...\n",
    "    text = ...\n",
    "\n",
    "    # remove stopwords\n",
    "    if rm_stop_words:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        # Apply tokenization and filter stop words\n",
    "        ...\n",
    "        text_list = ...\n",
    "        text = ' '.join(text_list)\n",
    "        \n",
    "    # remove short words\n",
    "    if rm_short_words:\n",
    "        # Apply tokenization and filter short words\n",
    "        ...\n",
    "        text_list = ...\n",
    "        # Put text back together\n",
    "        text = ' '.join(text_list)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5369bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(clean_text(ng_train.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e7baf5-129b-4ec2-8fe9-5892d8b70b10",
   "metadata": {},
   "source": [
    "The dataset contains 20 classes. However, **some of them are pretty close together. We aggregate them into 6 semantically coherent classes** which should not be easier to distinguish:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_labels(label):\n",
    "    # comp\n",
    "    if label in [1,2,3,4,5]:\n",
    "        new_label = 0\n",
    "    # rec\n",
    "    if label in [7,8,9,10]:\n",
    "        new_label = 1\n",
    "    # sci\n",
    "    if label in [11,12,13,14]:\n",
    "        new_label = 2\n",
    "    # misc \n",
    "    if label in [6]:\n",
    "        new_label = 3\n",
    "    # pol\n",
    "    if label in [16,17,18]:\n",
    "        new_label = 4\n",
    "    # rel\n",
    "    if label in [0,15,19]:\n",
    "        new_label = 5\n",
    "    return new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b052e25c-ce4c-4d9a-8c2f-d637a4417e71",
   "metadata": {},
   "source": [
    "We check that **we don't have any empty document**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba3d31f-d802-4d84-af86-48302dc69802",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57954c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_text = ...\n",
    "ng_train_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd33eece",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_test = fetch_20newsgroups(subset='test',\n",
    "                             remove=('headers', 'footers', 'quotes')\n",
    "                            )\n",
    "\n",
    "ng_test_text = ...\n",
    "ng_test_labels = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318babae-44b6-4bc7-8870-47715732ec37",
   "metadata": {},
   "source": [
    "We may apply a **lemmatizer**. We can get one from ```NLTK```.\n",
    "If we want it to work, we need the **part-of-speech** information of the word: \n",
    "- *Meeting* will not have the same lemma if it's a verb or a noun ! \n",
    "    \n",
    "For that, we can use ```NLTK``` tools:\n",
    "- ```word_tokenize``` to cut the document into tokens,\n",
    "- ```pos_tag``` to obtain part-of-speech tags,\n",
    "- ```get_wordnet_pos``` is a mapping function that will allow us to get the full POS designation to be used by the lemmatizer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68510b3c-94bf-43b9-ac1d-e3eac1884db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f289916-777d-48b3-b154-1718f67da996",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default solution\n",
    "\n",
    "def preprocess_and_lemmatize(text):   \n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged_tokens]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f1d2a-6626-453b-8b62-29806f780f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_doc = preprocess_and_lemmatize(clean_text(ng_train.data[0]))\n",
    "pprint(lemmatized_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae256c1-d178-4712-a047-f22e4b8b91eb",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948905c-39ad-47f3-af73-aad7d160ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ng_train_text_lemma = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f29c94-8df3-4f33-bee7-28f240812e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ng_train_text_splt, ng_val_text, ng_train_labels_splt, ng_val_labels = train_test_split(ng_train_text_lemma, ng_train_labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe9f3e1-2609-4bfa-93f3-0dde02fd8303",
   "metadata": {},
   "source": [
    "## 2 - Document representations \n",
    "\n",
    "Our statistical model, like most models applied to textual data, uses counts of word occurrences in a document. Thus, a very convenient way to represent a document is to use a Bag-of-Words (BoW) vector, containing the counts of each word (regardless of their order of occurrence) in the document. \n",
    "\n",
    "If we consider the set of all the words appearing in our $T$ training documents, which we note $V$ (Vocabulary), we can create **an index**, which is a bijection associating to each $w$ word an integer, which will be its position in $V$. \n",
    "\n",
    "Thus, for a document extracted from a set of documents containing $|V|$ different words, a BoW representation will be a vector of size $|V|$, whose value at the index of a word $w$ will be its number of occurrences in the document. \n",
    "\n",
    "We can use the **CountVectorizer** class from scikit-learn to better understand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a1a120-64c1-464c-a3e9-c8ef6dd7d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de060d5-0cbb-47a4-9916-8b48f8151e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['I walked down down the boulevard',\n",
    "          'I walked down the avenue',\n",
    "          'I ran down the boulevard',\n",
    "          'I walk down the city',\n",
    "          'I walk down the the avenue']\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "Bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68299479-12bc-46e6-bb12-38b99d736e57",
   "metadata": {},
   "source": [
    "We display the list containing the words ordered according to their index (Note that words of 2 characters or less are not counted)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5f4c1-632a-44bf-9e02-0aad9706ae60",
   "metadata": {},
   "source": [
    "The next function takes as input a list of documents (each in the form of a string) and returns, as in the example using ``CountVectorizer``:\n",
    "- A vocabulary that associates, to each word encountered, an index\n",
    "- A matrix, with rows representing documents and columns representing words indexed by the vocabulary. In position $(i,j)$, one should have the number of occurrences of the word $j$ in the document $i$.\n",
    "\n",
    "The vocabulary, which was in the form of a *list* in the previous example, can be returned in the form of a *dictionary* whose keys are the words and values are the indices. Since the vocabulary lists the words in the corpus without worrying about their number of occurrences, it can be built up using a set (in python).\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8d776-d694-4377-ab8f-1675db87559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    # Obtain the set of all words present in the data\n",
    "    ...\n",
    "    # Use it to create the vocabulary\n",
    "    vocabulary = dict(...) \n",
    "    # Create the term document matrix\n",
    "    counts = np.zeros((..., ...))\n",
    "    # Fill it \n",
    "    ...\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc3f85-fb7f-409a-881a-297662ab60ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Voc, X = count_words(corpus)\n",
    "print(Voc)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7129b64-86d9-4db5-8bc4-f2c0b543c8ec",
   "metadata": {},
   "source": [
    "Now, if we want to represent text that was not available when building the vocabulary, we will not be able to represent **new words** ! Let's take a look at how CountVectorizer does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6434cdcd-a808-43f5-ac79-3afadc4d8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_corpus = ['I walked up the street']\n",
    "Bow = vectorizer.transform(val_corpus)\n",
    "Bow.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1422ed1-6a63-4167-915d-7a860f8d6a65",
   "metadata": {},
   "source": [
    "Modify the ```count_words``` function to be able to deal with new documents when given a previously obtained vocabulary ! \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ae3a76-c83b-43c1-ad93-fe6cda53b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(texts, voc = None):\n",
    "    \"\"\"Vectorize text : return count of each word in the text snippets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : list of str\n",
    "        The texts\n",
    "    Returns\n",
    "    -------\n",
    "    vocabulary : dict\n",
    "        A dictionary that points to an index in counts for each word.\n",
    "    counts : ndarray, shape (n_samples, n_features)\n",
    "        The counts of each word in each text.\n",
    "    \"\"\"\n",
    "    if voc == None:\n",
    "        ...\n",
    "    else:\n",
    "        vocabulary = voc\n",
    "    ...\n",
    "    \n",
    "    return vocabulary, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4de517e-7553-475c-9e33-aabb5fc9abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc, train_bow = count_words(ng_train_text_splt)\n",
    "print(train_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbbad93-3cdd-49df-8e54-ea8ce351de53",
   "metadata": {},
   "source": [
    "Compare with the ```sklearn``` version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a8552c-c63d-423c-9721-0467035d7843",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "Bow = vectorizer.fit_transform(ng_train_text_splt)\n",
    "train_bow_sk = Bow.toarray()\n",
    "print(train_bow_sk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede322a-7574-47f3-9289-f956efcc9b8f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Careful: check the size that the representations are going to have (given the way they are build). What does this imply for the memory use ? What ```CountVectorizer``` arguments allows to avoid the issue ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badd214d-5fd3-4576-bbfe-ab8abc02b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, max_df=0.85)\n",
    "Bow = vectorizer.fit_transform(ng_train_text_splt)\n",
    "train_bow_sk = Bow.toarray()\n",
    "print(train_bow_sk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec73619-49f0-4892-ba87-8bca36b8a25a",
   "metadata": {},
   "source": [
    "In what comes next, we will mainly use the ```min_df``` and ```max_df``` arguments to affect pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930bb5a-c5db-47cc-b491-f69463777e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_bow = vectorizer.transform(ng_val_text).toarray()\n",
    "print(val_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf096ec-e920-4f67-a5c4-fa22d78cac65",
   "metadata": {},
   "source": [
    "Let's first look at the most frequent words. This will require some simple array manipulation:\n",
    "- Retrieving the sum of all word occurences across documents,\n",
    "- Sorting words according to their frequency,\n",
    "- Plotting an histogram for the top words, using the count as value and the word as legend.\n",
    "\n",
    "How can that influence our pre-processing ? \n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b7f816-2135-4768-80b9-657bc0596a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = ... # Total count of each word in the data\n",
    "top_words = ... # Indexes sorted by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9f427-214c-400b-90b7-1db646aa53d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary from the vectorizer using get_feature_names_out()\n",
    "voc = dict(zip(vectorizer.get_feature_names_out(),range(len(vectorizer.get_feature_names_out()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6bd69f-a19d-4b3d-9d82-f2d72ee927de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846ea95-eaf0-4869-9eb1-574da988131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_voc = {i: w for w, i in voc.items()} # Reverse vocabulary\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(15), frequency[top_words[:15]])\n",
    "ax.set_xticks(range(15))\n",
    "ax.set_xticklabels([rev_voc[i] for i in top_words[:15]], rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fac007-da9f-40a4-b53c-85c769225476",
   "metadata": {},
   "source": [
    "**Improving those representations with TF-DF**: This method is usually used to measure the importance of a term $i$ in a document $j$ relative to the rest of the corpus, from a matrix of occurrences $ words \\times documents$. Thus, for a matrix $\\mathbf{T}$ of $|V|$ terms and $D$ documents:\n",
    "$$\\text{TF}(T, w, d) = \\frac{T_{w,d}}{\\sum_{w'=1}^{|V|} T_{w',d}} $$\n",
    "\n",
    "$$\\text{IDF}(T, w) = \\log\\left(\\frac{D}{|\\{d : T_{w,d} > 0\\}|}\\right)$$\n",
    "\n",
    "$$\\text{TF-IDF}(T, w, d) = \\text{TF}(X, w, d) \\cdot \\text{IDF}(T, w)$$\n",
    "\n",
    "TF-IDF is generally better suited to low-density matrices, since it will penalize terms that appear in a large part of the documents. \n",
    "Implement a function transforming the BOW representations we obtained as output of ```count_words``` into TF-IDF representations:\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eace1f3-4858-425c-8644-b1d89910e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def tfidf_transform(bow):\n",
    "    \"\"\"\n",
    "    Inverse document frequencies applied to our bag-of-words representations\n",
    "    \"\"\"\n",
    "    # IDF\n",
    "    ...\n",
    "    idfs = ...\n",
    "    # TF\n",
    "    ...\n",
    "    tfs = ...\n",
    "    \n",
    "    tf_idf = tfs * np.expand_dims(idfs,axis=0)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9da48e-2348-415c-a456-201aede13449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = tfidf_transform(train_bow_sk)\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6b39b-5c55-47fa-a7e1-b0630484ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0d0bb-9ebc-4a05-92c9-2da511aaf0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the vectorizer to the training data\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2, max_df=0.85)\n",
    "Tfidf = tfidf_vectorizer.fit_transform(ng_train_text_splt)\n",
    "tfidf_sk = Tfidf.toarray()\n",
    "print(tfidf_sk.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c1ee94-bf96-4c73-ae4b-d405855a8df0",
   "metadata": {},
   "source": [
    "## 3 - Classification with Naive Bayesian \n",
    "\n",
    "We will implement a class ```NB``` that should correspond to a **scikit-learn model**. It will contain the following methods:\n",
    "\n",
    "```python\n",
    "def fit(self, X, y)\n",
    "``` \n",
    "**Training**: will learn a statistical model based on the representations $X$ corresponding to the labels $y$.\n",
    "Here, $X$ contains representations obtained as the output of ```count_words```. You can complete the function using the procedure detailed above. \n",
    "\n",
    "Note: the smoothing is not necessarily done with a $1$ - it can be done with a positive value $\\alpha$, which we can implement as an argument of the class ```NB```.\n",
    "\n",
    "```python\n",
    "def predict(self, X)\n",
    "```\n",
    "**Testing**: will return the labels predicted by the model for other representations $X$.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d51b83-ec74-40ff-afe1-c1ea9853c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NB(BaseEstimator, ClassifierMixin):\n",
    "    # Class arguments allow to use sklearn methods \n",
    "    def __init__(self, alpha=1.0):\n",
    "        # alpha is a smoothing parameter\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Compute the prior probabilities of classes\n",
    "        ...\n",
    "        # And the conditional probabilities of words given classes\n",
    "        ...\n",
    "        # Save them as model attributes\n",
    "        self.log_prior_ = ...\n",
    "        self.log_cond_prob_ = ...\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Do prediction: compute the score of each document\n",
    "        ...\n",
    "        scores = ...\n",
    "        # And return the classes maximizing those scores\n",
    "        ...\n",
    "        return ...\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        # Return accuracy\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b18601-afd6-4bc8-ad40-55c051100638",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_nb = NB()\n",
    "clf_nb.fit(train_bow_sk, ng_train_labels_splt)\n",
    "val_pred = clf_nb.predict(val_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d74e9-18db-446c-b2fe-e50cf7495bde",
   "metadata": {},
   "source": [
    "Besides accuracy, we can look at **F1**-measures, and display the *confusion matrix*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a13d3-9ea0-4f3a-9aaa-f0330f03e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4d8538-fb9e-4990-9be6-ae716b13f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ng_val_labels, val_pred))\n",
    "cm = confusion_matrix(ng_val_labels , val_pred, normalize='true')\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(6))\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d44de-a5b2-4de6-8bad-445c658588b5",
   "metadata": {},
   "source": [
    "We can also use the scikit-learn ```MultinomialNB```. Experiment on this model too and compare the results.\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d390563d-db61-4775-981f-7c221b03d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Fit the model on the training data\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506bfd1-8615-4916-bc69-1a998da39b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the prediction and evaluation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e900693e-2fc0-42bd-a2e5-e8b2c6618877",
   "metadata": {},
   "source": [
    "We want to **find the best hyper-parameters** for our model: in this case, it will mainly affect the pre-processing.\n",
    "In what follows, use ```Pipeline``` to perform a series of quick experiments, and use the validation data to check which set of representations (depending on ```min_df```, ```max_df``` and using ```tf-idf``` or not):\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a6043-8010-4e46-99d1-bba03fb60075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a5316-fb67-4810-b246-b089bbcf169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline_tfidf = Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d902d-05fc-4f88-9182-a11d1e536b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dfs = [1, 2, 3, 5, 10]\n",
    "max_dfs = [0.5, 0.6, 0.7, 0.85, 1.0]\n",
    "\n",
    "# Test the model for those pre-processing hyper-parameters\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5026fb-bb9c-4a4d-9c11-4cde8fa8ae7b",
   "metadata": {},
   "source": [
    "## 4 - Topic modeling with Latent Dirichlet Allocation\n",
    "\n",
    "We will now investigate the use of Latent Semantic Analysis  and Latent Dirichlet Allocation for topic modeling.\n",
    "Let's begin with a simple application of both methods with a reduced number of topics (*e.g*, ```n_topics = 20```) and try to interpret them. \n",
    "- We will use ```TruncatedSVD``` for LSA and ```LatentDirichletAllocation``` for LDA\n",
    "- We will look at the most important words for each topic\n",
    "- We will visualize the topics with ```pyLDAvis```\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8980499-dde1-4bb7-8509-0f83556a465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d747a0-055f-4829-8056-24cc37e6e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take the best configuration obtained for classification\n",
    "vectorizer = CountVectorizer(min_df=..., max_df=...)\n",
    "Bow = vectorizer.fit_transform(ng_train_text_lemma)\n",
    "train_bow_tm = Bow.toarray()\n",
    "print(train_bow_tm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a52d69-60e4-45bd-a51d-48d10e314549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty documents (with that pre-processing)\n",
    "mask = (train_bow_tm.sum(axis=1) > 0)\n",
    "train_bow_tm = train_bow_tm[mask]\n",
    "print(train_bow_tm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975cd5d-ff70-4bb0-a615-a9f53a36bb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa = TruncatedSVD(n_components = 20)\n",
    "lsa_train_topics = lsa.fit_transform(train_bow_tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa7de8-5941-4f9b-a74c-7685505c4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspondances between documents and topics\n",
    "print(lsa_train_topics.shape)\n",
    "# Correspondances between topics and words\n",
    "print(lsa.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b11bbc9-0970-4027-bad6-c38dd163b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = dict(zip(vectorizer.get_feature_names_out(),range(len(vectorizer.get_feature_names_out()))))\n",
    "rev_voc = {i: w for w, i in voc.items()}\n",
    "\n",
    "def most_important_words(n, reverse_vocabulary, topic_model):\n",
    "    out = []\n",
    "    for i, topic in enumerate(topic_model.components_):\n",
    "        out.append([reverse_vocabulary[j] for j in topic.argsort()[:-n-1:-1]])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded8a49a-4062-460e-b006-cab0b0e23c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc, lsa)\n",
    "for i, topic in enumerate(words[:]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e919149d-7824-49a9-ac53-320f46be6b04",
   "metadata": {},
   "source": [
    "To use ```pyLDAvis```, we need **probability distributions**. We will need to adapt the result of LSA. However, it will be very easy with LDA ! \n",
    "\n",
    "How to perform this adaptation ? What do you think is the best way to transform real scores in a probability distribution in this context ? \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ee62e-0822-4987-9b3b-4ddd344f04b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca576b10-b349-442e-953e-30b9950b94dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution for topic / word correspondance\n",
    "train_topic_term_abs = ...\n",
    "train_topic_term_prob = train_topic_term_abs / train_topic_term_abs.sum(axis=1)[:, None]\n",
    "\n",
    "# Distribution for document / topic correspondance \n",
    "train_doc_topic_abs = ...\n",
    "train_doc_topic_prob = train_doc_topic_abs / train_doc_topic_abs.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed45c4d-ccfd-4b28-afdd-2c6496d9803b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prepared_data = pyLDAvis.prepare(\n",
    "    topic_term_dists=train_topic_term_prob,\n",
    "    doc_topic_dists=train_doc_topic_prob,\n",
    "    doc_lengths=train_bow_tm.sum(axis=1),\n",
    "    vocab=vectorizer.get_feature_names_out(),\n",
    "    term_frequency=train_bow_tm.sum(axis = 0)\n",
    ")\n",
    "\n",
    "pyLDAvis.display(prepared_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d276743-7642-4163-8dd9-4e4c7bf628b7",
   "metadata": {},
   "source": [
    "We do the same for ```LDA```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e83e08-ca27-4be9-b526-981ff991e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components = 20)\n",
    "lda_train_topics = lda.fit_transform(train_bow_tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b515038-c64c-426a-b07b-41d503012393",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = most_important_words(8, rev_voc, lda)\n",
    "for i, topic in enumerate(words[:]):\n",
    "    print(\"Topic \", i+1, \" : \", topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dce404-1b86-4573-82a4-6f48bcf59c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.lda_model\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f0647-4a50-448d-b7b8-bc5fe75ee683",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.lda_model.prepare(lda, Bow, vectorizer)\n",
    "# Look at https://nbviewer.org/github/bmabey/pyLDAvis/blob/master/notebooks/LDA%20model.ipynb for an example of \n",
    "# application to a sklearn LDA model. Look at the different multidimensional scaling options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e74f2ab-69e1-4947-91e2-8391bf4b5be1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We can now implement two (imperfect) metrics to try to check how our topic models are behaving:\n",
    "- **Topic diversity**, looking at how redundant top-words in our topics are,\n",
    "    - Let's define it as the *proportion of unique words* in top words of topics \n",
    "- **Topic coherence**, looking at how top-words in our topics actually co-occur in the data.\n",
    "    - We will look at the proportion of documents which containt pairs of top-words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a822ad7-962a-4e59-af69-be8b7b085f69",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef4ca8e-e1f5-4b8e-9a02-cf61d164bbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_diversity(components, top_n=10):\n",
    "    top_words = []\n",
    "    for topic in components:\n",
    "        # Index of top_n words in that topic\n",
    "        top_indices = ...\n",
    "        top_words.extend(top_indices)\n",
    "    # Compute the proportion of unique words\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8797b5-4057-4516-abcf-c2a7290dba50",
   "metadata": {},
   "source": [
    "What are the range of values taken by this measure ? How to interpret it ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47637647-b785-4b8b-8c71-7bf5d4c680f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(topic_diversity(lsa.components_))\n",
    "print(topic_diversity(lda.components_))\n",
    "# Value between 0 and 1, more diverse when close to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc6d67-499f-48f8-9d6e-a8b21175ff70",
   "metadata": {},
   "source": [
    "Topic coherence is applied to the **binary** term-document matrix:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7af0fe-03b7-454d-a844-fcb1e3f2b7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bow_binary = (train_bow_tm > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b26335-1b72-4366-9c98-eaaa5bf013e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def umass_coherence(components, bow_binary, top_n=10):\n",
    "    scores = []\n",
    "    \n",
    "    for topic in components:\n",
    "        top_words = topic.argsort()[-top_n:]\n",
    "        score = 0\n",
    "        for i in range(1, len(top_words)):\n",
    "            for j in range(i):\n",
    "                D_wi_wj = np.sum(bow_binary[:, top_words[i]] * bow_binary[:, top_words[j]])\n",
    "                D_wj = np.sum(bow_binary[:, top_words[j]])\n",
    "                score += np.log((D_wi_wj + 1) / D_wj)\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c7f52-f5a3-4ea5-9bf2-d3107cbd65f3",
   "metadata": {},
   "source": [
    "What are the range of values taken by this measure ? How to interpret it ?\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b94a6-9fe9-48d8-98ed-20a96bc360b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"UMass coherence:\", umass_coherence(lsa.components_, train_bow_binary))\n",
    "print(\"UMass coherence:\", umass_coherence(lda.components_, train_bow_binary))\n",
    "# Negative value, should get close to 0 when there is perfect co-occurence of the top words in each topic.\n",
    "# Very negative -> topics are \"more separated\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd464b-a120-4f55-9530-9d908a55249c",
   "metadata": {},
   "source": [
    "Vary the number of topics ```n_topics``` for the LDA model, and find out which seems to be giving the best results:\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89b6a4-2d20-40b9-b02c-df693e0b260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b17d2-8b1c-4e69-aa1c-607719f39d3d",
   "metadata": {},
   "source": [
    "Investigate using the document representations in **topic space** for the classification task. Search for the best number of topics, performance-wise. Is it the same than before ? \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d7a3b0-915b-4ae9-ad37-cca80d2435f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668ac319-047a-438d-b924-85a7316f734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4656a5a0-d240-4a73-a715-ce43208e63f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expect those results to change with pre-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
